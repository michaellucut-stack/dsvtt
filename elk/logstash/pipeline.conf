# =============================================================================
# DSVTT — Logstash Pipeline Configuration (Enhanced for Sprint 7)
# =============================================================================
# Routes logs to dedicated indices based on context:
#   - dsvtt-security-*  — security audit events
#   - dsvtt-game-*      — game/room/lobby events
#   - dsvtt-server-*    — general server logs
# All indices use daily rotation (YYYY.MM.dd suffix).
# =============================================================================

input {
  # Primary input: JSON-formatted application logs over TCP
  tcp {
    port  => 5044
    codec => json
    add_field => { "[@metadata][input]" => "tcp" }
  }

  # Beat-style input for filebeat/metricbeat if deployed
  beats {
    port => 5045
    add_field => { "[@metadata][input]" => "beats" }
  }
}

# =============================================================================
# Filter Chain
# =============================================================================
filter {

  # -------------------------------------------------------------------------
  # 1. Parse JSON body — handle cases where the log arrives as a raw string
  #    wrapped in a "message" field rather than pre-parsed JSON.
  # -------------------------------------------------------------------------
  if [message] and ![level] {
    json {
      source       => "message"
      target       => "parsed"
      skip_on_invalid_json => true
    }

    # Promote parsed fields to top level if JSON parsing succeeded
    if [parsed] {
      mutate {
        rename => {
          "[parsed][level]"     => "level"
          "[parsed][message]"   => "log_message"
          "[parsed][context]"   => "context"
          "[parsed][requestId]" => "requestId"
          "[parsed][timestamp]" => "timestamp"
          "[parsed][service]"   => "service"
          "[parsed][clientIp]"  => "clientIp"
          "[parsed][userId]"    => "userId"
          "[parsed][method]"    => "method"
          "[parsed][path]"      => "path"
          "[parsed][statusCode]"=> "statusCode"
          "[parsed][duration]"  => "duration"
          "[parsed][userAgent]" => "userAgent"
          "[parsed][wsConnections]" => "wsConnections"
        }
        remove_field => ["parsed"]
      }
    }
  }

  # -------------------------------------------------------------------------
  # 2. Extract core fields — ensure level, service, and context always exist
  # -------------------------------------------------------------------------
  if ![service] {
    mutate {
      add_field => { "service" => "dsvtt-server" }
    }
  }

  if ![context] {
    mutate {
      add_field => { "context" => "general" }
    }
  }

  # -------------------------------------------------------------------------
  # 3. Timestamp parsing — use the application-provided timestamp when
  #    available; fall back to Logstash ingest time (@timestamp default).
  # -------------------------------------------------------------------------
  if [timestamp] {
    date {
      match        => ["timestamp", "ISO8601", "yyyy-MM-dd'T'HH:mm:ss.SSSZ"]
      target       => "@timestamp"
      remove_field => ["timestamp"]
    }
  }

  # -------------------------------------------------------------------------
  # 4. Normalize log level to lowercase for consistent querying
  # -------------------------------------------------------------------------
  if [level] {
    mutate {
      lowercase => ["level"]
    }
  }

  # -------------------------------------------------------------------------
  # 5. GeoIP lookup — resolve geographic location from client IP addresses.
  #    Populates [geoip] with lat/lon, country, city, etc.
  # -------------------------------------------------------------------------
  if [clientIp] {
    geoip {
      source   => "clientIp"
      target   => "geoip"
      database => "/usr/share/logstash/vendor/GeoLite2-City.mmdb"
      tag_on_failure => ["_geoip_lookup_failure"]
    }
  }

  # -------------------------------------------------------------------------
  # 6. Tag security events — any log with context "security-audit" gets
  #    routed to the dedicated security index and tagged for alerting.
  # -------------------------------------------------------------------------
  if [context] == "security-audit" {
    mutate {
      add_tag   => ["security-event"]
      add_field => { "[@metadata][target_index]" => "dsvtt-security" }
    }

    # Extract authentication-specific fields for security dashboards
    if [log_message] =~ /(?i)failed.*auth|login.*fail|brute.?force|unauthorized/ {
      mutate {
        add_tag => ["auth-failure"]
      }
    }

    if [log_message] =~ /(?i)token.*invalid|token.*expired|jwt.*error/ {
      mutate {
        add_tag => ["token-error"]
      }
    }

  # -------------------------------------------------------------------------
  # 7. Tag game events — game, room, and lobby contexts route to the
  #    game-specific index for gameplay analytics.
  # -------------------------------------------------------------------------
  } else if [context] == "game" or [context] == "room" or [context] == "lobby" or [context] == "match" {
    mutate {
      add_tag   => ["game-event"]
      add_field => { "[@metadata][target_index]" => "dsvtt-game" }
    }

  # -------------------------------------------------------------------------
  # 8. Everything else is a general server log
  # -------------------------------------------------------------------------
  } else {
    mutate {
      add_tag   => ["server-log"]
      add_field => { "[@metadata][target_index]" => "dsvtt-server" }
    }
  }

  # -------------------------------------------------------------------------
  # 9. HTTP status code classification — add tags for error-rate alerting
  # -------------------------------------------------------------------------
  if [statusCode] {
    # Convert to integer for range comparisons
    mutate {
      convert => { "statusCode" => "integer" }
    }

    if [statusCode] >= 500 {
      mutate { add_tag => ["http-5xx"] }
    } else if [statusCode] >= 400 {
      mutate { add_tag => ["http-4xx"] }
    } else if [statusCode] >= 200 and [statusCode] < 300 {
      mutate { add_tag => ["http-2xx"] }
    }
  }

  # -------------------------------------------------------------------------
  # 10. Duration normalization — ensure duration is numeric (milliseconds)
  # -------------------------------------------------------------------------
  if [duration] {
    mutate {
      convert => { "duration" => "float" }
    }
  }

  # -------------------------------------------------------------------------
  # 11. WebSocket connections — ensure numeric type for gauge visualizations
  # -------------------------------------------------------------------------
  if [wsConnections] {
    mutate {
      convert => { "wsConnections" => "integer" }
    }
  }

  # -------------------------------------------------------------------------
  # 12. Fingerprint for deduplication — use requestId when available
  # -------------------------------------------------------------------------
  if [requestId] {
    fingerprint {
      source => ["requestId"]
      target => "[@metadata][fingerprint]"
      method => "SHA256"
    }
  }
}

# =============================================================================
# Output — route to the appropriate daily index
# =============================================================================
output {

  # Primary output: Elasticsearch with date-based index naming
  elasticsearch {
    hosts    => ["http://elasticsearch:9200"]
    # Daily rotation: dsvtt-{security|game|server}-YYYY.MM.dd
    index    => "%{[@metadata][target_index]}-%{+YYYY.MM.dd}"
    # Use ILM policy for lifecycle management
    ilm_enabled   => true
    ilm_rollover_alias => "%{[@metadata][target_index]}"
    ilm_policy    => "dsvtt-ilm-policy"
    # Dedup using requestId fingerprint when available
    document_id   => "%{[@metadata][fingerprint]}"
  }

  # Debug output — stdout with rubydebug codec (disable in production)
  stdout {
    codec => rubydebug
  }
}
